{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d48f6f4b-4eb6-4361-b0e8-3225f2c089ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Claudio\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "path = '../datasets/'\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os_elmOTENIM import OS_ELM\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f6c49a-d29a-4193-94a5-e912f24069cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Otenim(n_input_nodes,n_hidden_nodes,n_output_nodes,X,y,f):\n",
    "    \n",
    "    os_elm = OS_ELM(\n",
    "        n_input_nodes=n_input_nodes,\n",
    "        n_hidden_nodes=n_hidden_nodes,\n",
    "        n_output_nodes=n_output_nodes,\n",
    "        loss='mean_squared_error',\n",
    "        activation=f,\n",
    "    )\n",
    "    \n",
    "    n_classes = n_output_nodes\n",
    "    x_train, x_test, t_train, t_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "    \n",
    "    border = int(1.2 * n_hidden_nodes)\n",
    "    x_train_init = x_train[:border]\n",
    "    x_train_seq = x_train[border:]\n",
    "    t_train_init = t_train[:border]\n",
    "    t_train_seq = t_train[border:]\n",
    "    \n",
    "    start = timer ()\n",
    "    os_elm.init_train(x_train_init, t_train_init)\n",
    "    \n",
    "    batch_size = 1\n",
    "    \n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        t_batch = t_train_seq[i:i+batch_size]\n",
    "        os_elm.seq_train(x_batch, t_batch)\n",
    "    \n",
    "    end = timer ()\n",
    "    time = end - start\n",
    "    #print(\"TIME\", end - start)\n",
    "    pred = os_elm.predict(x_test)\n",
    "    y_ = softmax(pred)\n",
    "    yy = []\n",
    "    for i in y_:\n",
    "        yy.append(np.argmax(i))\n",
    "    tt = []\n",
    "    for i in t_test:\n",
    "        tt.append(np.argmax(i))\n",
    "    #cm = confusion_matrix(tt, yy)\n",
    "    #sns.heatmap(cm, annot=True)\n",
    "    f1,a,p,r = metrics(tt,yy)\n",
    "    \n",
    "    return os_elm,time,f1,a,p,r\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a, axis=-1).reshape(-1, 1)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a, axis=-1).reshape(-1, 1)\n",
    "    return exp_a / sum_exp_a\n",
    "\n",
    "def metrics(true,predicted):\n",
    "    p = precision_score(true,predicted,average='macro',labels=np.unique(predicted))\n",
    "    r = recall_score(true,predicted,average='macro')\n",
    "    a = accuracy_score(true,predicted)\n",
    "    f1 = f1_score(true,predicted,average='macro')\n",
    "\n",
    "    return f1,a,p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b627d27-94ae-423d-8ccc-289b8f9a3af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hidden_nodes = 25 SIGMOID\n",
      "Tiempo: 0.06462141200000132\n",
      "F1: 0.4292221784484774\n",
      "A: 0.45376344086021514\n",
      "P: 0.4292568259844036\n",
      "R: 0.450915602013163\n",
      "\n",
      "n_hidden_nodes = 50 SIGMOID\n",
      "Tiempo: 0.058236414000001846\n",
      "F1: 0.42908921035719105\n",
      "A: 0.4507526881720431\n",
      "P: 0.42738110390699696\n",
      "R: 0.44628242353852116\n",
      "\n",
      "n_hidden_nodes = 100 SIGMOID\n",
      "Tiempo: 0.04254697399999884\n",
      "F1: 0.4168293026569449\n",
      "A: 0.44193548387096776\n",
      "P: 0.41360739894577436\n",
      "R: 0.4374128919860627\n",
      "\n",
      "n_hidden_nodes = 25 RELU\n",
      "Tiempo: 0.06439058999999986\n",
      "F1: 0.4343608452237421\n",
      "A: 0.4556989247311829\n",
      "P: 0.4375339202903934\n",
      "R: 0.4528184281842818\n",
      "\n",
      "n_hidden_nodes = 50 RELU\n",
      "Tiempo: 0.058606801999998764\n",
      "F1: 0.45149424930549353\n",
      "A: 0.4679569892473118\n",
      "P: 0.45417786946839583\n",
      "R: 0.46841753774680606\n",
      "\n",
      "n_hidden_nodes = 100 RELU\n",
      "Tiempo: 0.04286715400000162\n",
      "F1: 0.4588948541498382\n",
      "A: 0.47440860215053765\n",
      "P: 0.46179671422892327\n",
      "R: 0.4747531939605111\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Pokemon.csv\"\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "data = pd.read_csv(path + dataset)\n",
    "\n",
    "\n",
    "n_input_nodes = 10 #CANTIDAD DE ATRIBUTOS\n",
    "n_output_nodes = 3 #CANTIDAD DE CLASES\n",
    "\n",
    "\n",
    "data = data.set_index(\"#\")\n",
    "\n",
    "d = {}\n",
    "for i in range(len(data[\"Type 2\"].unique())):\n",
    "  d[data[\"Type 2\"].unique()[i]] = i + 1\n",
    "data = data.drop(columns = [\"Name\"])\n",
    "data = data.fillna(0)\n",
    "data[\"Type 2\"] = data[\"Type 2\"].replace(d)\n",
    "\n",
    "\n",
    "data_balanced = data[(data[\"Type 1\"] == \"Water\") | (data[\"Type 1\"] == \"Normal\") | (data[\"Type 1\"] == \"Grass\")]\n",
    "m = {'Water':0,'Normal':1,'Grass':2}\n",
    "data_balanced = data_balanced.replace({'Type 1':m})\n",
    "y = data_balanced[\"Type 1\"].to_numpy()\n",
    "data_balanced.drop(['Type 1'],inplace=True,axis=1)\n",
    "\n",
    "means = np.mean(data_balanced,axis=0)\n",
    "stds = np.std(data_balanced,axis=0)\n",
    "data_balanced = (data_balanced-means)/stds\n",
    "\n",
    "X = data_balanced.to_numpy()\n",
    "t = to_categorical(y, num_classes=n_output_nodes) \n",
    "\n",
    "\n",
    "\n",
    "n_hidden_nodes = 25\n",
    "tiempo = []\n",
    "f1 = []\n",
    "a = []\n",
    "p = []\n",
    "r = []\n",
    "\n",
    "for i in range(50):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    model,t_,f1_,a_,p_,r_ = Otenim(n_input_nodes,n_hidden_nodes,n_output_nodes,X,t,\"sigmoid\")\n",
    "    tiempo.append(t_)\n",
    "    f1.append(f1_)\n",
    "    a.append(a_)\n",
    "    p.append(p_)\n",
    "    r.append(r_)\n",
    "print(\"\\nn_hidden_nodes =\",n_hidden_nodes , \"SIGMOID\")\n",
    "print(\"Tiempo:\",np.mean(tiempo))\n",
    "print(\"F1:\",np.mean(f1))\n",
    "print(\"A:\",np.mean(a))\n",
    "print(\"P:\",np.mean(p))\n",
    "print(\"R:\",np.mean(r))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_hidden_nodes = 50\n",
    "tiempo = []\n",
    "f1 = []\n",
    "a = []\n",
    "p = []\n",
    "r = []\n",
    "\n",
    "for i in range(50):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    model,t_,f1_,a_,p_,r_ = Otenim(n_input_nodes,n_hidden_nodes,n_output_nodes,X,t,\"sigmoid\")\n",
    "    tiempo.append(t_)\n",
    "    f1.append(f1_)\n",
    "    a.append(a_)\n",
    "    p.append(p_)\n",
    "    r.append(r_)\n",
    "print(\"\\nn_hidden_nodes =\",n_hidden_nodes , \"SIGMOID\")\n",
    "print(\"Tiempo:\",np.mean(tiempo))\n",
    "print(\"F1:\",np.mean(f1))\n",
    "print(\"A:\",np.mean(a))\n",
    "print(\"P:\",np.mean(p))\n",
    "print(\"R:\",np.mean(r))\n",
    "\n",
    "\n",
    "\n",
    "n_hidden_nodes = 100\n",
    "tiempo = []\n",
    "f1 = []\n",
    "a = []\n",
    "p = []\n",
    "r = []\n",
    "\n",
    "for i in range(50):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    model,t_,f1_,a_,p_,r_ = Otenim(n_input_nodes,n_hidden_nodes,n_output_nodes,X,t,\"sigmoid\")\n",
    "    tiempo.append(t_)\n",
    "    f1.append(f1_)\n",
    "    a.append(a_)\n",
    "    p.append(p_)\n",
    "    r.append(r_)\n",
    "print(\"\\nn_hidden_nodes =\",n_hidden_nodes , \"SIGMOID\")\n",
    "print(\"Tiempo:\",np.mean(tiempo))\n",
    "print(\"F1:\",np.mean(f1))\n",
    "print(\"A:\",np.mean(a))\n",
    "print(\"P:\",np.mean(p))\n",
    "print(\"R:\",np.mean(r))\n",
    "\n",
    "\n",
    "n_hidden_nodes = 25\n",
    "tiempo = []\n",
    "f1 = []\n",
    "a = []\n",
    "p = []\n",
    "r = []\n",
    "\n",
    "for i in range(50):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    model,t_,f1_,a_,p_,r_ = Otenim(n_input_nodes,n_hidden_nodes,n_output_nodes,X,t,\"relu\")\n",
    "    tiempo.append(t_)\n",
    "    f1.append(f1_)\n",
    "    a.append(a_)\n",
    "    p.append(p_)\n",
    "    r.append(r_)\n",
    "print(\"\\nn_hidden_nodes =\",n_hidden_nodes , \"RELU\")\n",
    "print(\"Tiempo:\",np.mean(tiempo))\n",
    "print(\"F1:\",np.mean(f1))\n",
    "print(\"A:\",np.mean(a))\n",
    "print(\"P:\",np.mean(p))\n",
    "print(\"R:\",np.mean(r))\n",
    "\n",
    "\n",
    "\n",
    "n_hidden_nodes = 50\n",
    "tiempo = []\n",
    "f1 = []\n",
    "a = []\n",
    "p = []\n",
    "r = []\n",
    "\n",
    "for i in range(50):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    model,t_,f1_,a_,p_,r_ = Otenim(n_input_nodes,n_hidden_nodes,n_output_nodes,X,t,\"relu\")\n",
    "    tiempo.append(t_)\n",
    "    f1.append(f1_)\n",
    "    a.append(a_)\n",
    "    p.append(p_)\n",
    "    r.append(r_)\n",
    "print(\"\\nn_hidden_nodes =\",n_hidden_nodes , \"RELU\")\n",
    "print(\"Tiempo:\",np.mean(tiempo))\n",
    "print(\"F1:\",np.mean(f1))\n",
    "print(\"A:\",np.mean(a))\n",
    "print(\"P:\",np.mean(p))\n",
    "print(\"R:\",np.mean(r))\n",
    "\n",
    "\n",
    "n_hidden_nodes = 100\n",
    "tiempo = []\n",
    "f1 = []\n",
    "a = []\n",
    "p = []\n",
    "r = []\n",
    "\n",
    "for i in range(50):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    model,t_,f1_,a_,p_,r_ = Otenim(n_input_nodes,n_hidden_nodes,n_output_nodes,X,t,\"relu\")\n",
    "    tiempo.append(t_)\n",
    "    f1.append(f1_)\n",
    "    a.append(a_)\n",
    "    p.append(p_)\n",
    "    r.append(r_)\n",
    "print(\"\\nn_hidden_nodes =\",n_hidden_nodes , \"RELU\")\n",
    "print(\"Tiempo:\",np.mean(tiempo))\n",
    "print(\"F1:\",np.mean(f1))\n",
    "print(\"A:\",np.mean(a))\n",
    "print(\"P:\",np.mean(p))\n",
    "print(\"R:\",np.mean(r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
