{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90266713-d849-4fff-bf2b-6d419b222993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, Dense\n",
    "from tensorflow.keras.layers import Activation, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from sklearn import preprocessing\n",
    "# Resets \n",
    "tf.compat.v1.reset_default_graph()\n",
    "path = \"../datasets/\"\n",
    "\n",
    "n_input_nodes = 100 \n",
    "n_output_nodes = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904760e9-5345-4961-a0a0-205917100e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSquareDeviation(predictions, truth):\n",
    "  squareDeviation = np.square(predictions-truth)\n",
    "  return squareDeviation\n",
    "\n",
    "\n",
    "# Function that initializes weights and biases for a given layer of the network architecture.\n",
    "def layer(inputs,neurons,layer_name):\n",
    "    # We need to initialize the weights with the constraint of a standard deviation\n",
    "    weights = tf.Variable(tf.random.normal((inputs.shape[1],neurons), stddev = 1/neurons ), name = layer_name)\n",
    "    # With bias we don't need to worry about this as it is used mainly to recenter the function line to the origin\n",
    "    bias = tf.Variable(tf.zeros([neurons]), name = layer_name)\n",
    "    return weights,bias\n",
    "\n",
    "# Function that receives the inputs and network architecture and initializes every layer.\n",
    "def create_network(X,layers):\n",
    "    network = []\n",
    "    variables = []\n",
    "    # First previous are the data inputs.\n",
    "    previous = X\n",
    "    for ix, neurons in enumerate(layers):\n",
    "        weights,bias = layer(previous,neurons,f'layer_{ix}')\n",
    "        network.append( (weights,bias) )\n",
    "        variables.extend( (weights,bias) )\n",
    "        # Chain weights to next layer.\n",
    "        previous = weights\n",
    "    return network, variables\n",
    "\n",
    "# Function that will iterate through the network architecture and apply the linear (multiplication of the weight and sum of the bias) and non linear (apply the Leaky ReLU activation) tranformations.\n",
    "def predict(X, network,f):\n",
    "    net = X\n",
    "    layer = 1\n",
    "    # For every layer except the output one (last one) apply the linear transformation and activation\n",
    "    for weights,bias in network[:-1]:\n",
    "        with tf.name_scope(f'Layer_{layer}'):\n",
    "            net = tf.add(tf.matmul(net, weights), bias,name='net')\n",
    "            if f == \"relu\":\n",
    "                net = tf.nn.relu(net, name=\"relu\")\n",
    "            else:\n",
    "                net = tf.nn.sigmoid(net, name=\"sigmoid\")\n",
    "        layer += 1\n",
    "    weights,bias = network[-1]\n",
    "    # Output layer\n",
    "    with tf.name_scope('Output'):\n",
    "        net = tf.add(tf.matmul(net, weights), bias)\n",
    "    return net\n",
    "\n",
    "\n",
    "# Loss function that will calculate the quadratic error between a predicted value and it's target.\n",
    "def mean_squared_error(predicted, y):\n",
    "    cost = tf.reduce_mean(tf.math.square(y-predicted))\n",
    "    return cost\n",
    "\n",
    "# Function that will create the GradientTape object that will trace the computations and compute the derivatives.\n",
    "# Receive variables because they already are in a list (variable, gradient)\n",
    "def grad(X, Y, network, variables,f):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted = predict(X, network,f)\n",
    "        loss_val = mean_squared_error(predicted,Y)\n",
    "    return tape.gradient(loss_val, variables),variables\n",
    "\n",
    "        \n",
    "def AndreTensor(X,Y,size,steps,n_input_nodes,n_hidden_nodes,n_output_nodes,valid_X,valid_Y,f,use_earlyStopFunction):\n",
    "    # Define net architecture\n",
    "    layers=[n_input_nodes,n_hidden_nodes,n_output_nodes]\n",
    "    # Define batch size\n",
    "    batch_size = size\n",
    "\n",
    "    # Get the network layer initialization\n",
    "    network, variables = create_network(X,layers)\n",
    "    # Calculate the batches per epoch\n",
    "    batches_per_epoch = X.shape[0]//batch_size\n",
    "    #batches_per_epoch = 1\n",
    "    learning_rate = 0.0001\n",
    "    # Initialize the optimizer to be used.\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "\n",
    "    # Number of epochs to be trained\n",
    "    epochs = steps\n",
    "    er_ = []\n",
    "    # Iterate through the epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\r\",epoch,\"/\",epochs,end=\"\")\n",
    "        # Get indexes for batch row choice\n",
    "        shuffled = np.arange(len(Y))\n",
    "        # Randomize these indexes\n",
    "        np.random.shuffle(shuffled)\n",
    "        # Iterate through the batches\n",
    "        for batch_num in range(batches_per_epoch):\n",
    "            # Get batch starting position\n",
    "            start = batch_num*batch_size\n",
    "            # Get training inputs batch and labels input batch, also turn them into float32 as it is numerically safer\n",
    "            batch_xs = tf.constant(X[shuffled[start:start+batch_size],:].astype(np.float32))\n",
    "            batch_ys = tf.constant(Y[shuffled[start:start+batch_size]].astype(np.float32))\n",
    "            # Go get the gradients to be computed\n",
    "            gradients,variables = grad(batch_xs, batch_ys, network, variables,f)\n",
    "            # Compute the gradients using the optimizer and update network\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "        train_pred = predict(tf.constant(X.astype(np.float32)), network,f)\n",
    "        # Predict the labels for the whole input validation list \n",
    "        val_pred = predict(tf.constant(valid_X.astype(np.float32)), network,f)\n",
    "\n",
    "        # Compute the quadratic error for train and test predicted sets\n",
    "        train_error = mean_squared_error(train_pred, Y)\n",
    "        val_error = mean_squared_error(val_pred, valid_Y)\n",
    "        if(use_earlyStopFunction and epoch >= 2):\n",
    "            ar = er_[-3:]\n",
    "            if(val_error >= ar[0] and val_error >= ar[1] and val_error >= ar[2]):\n",
    "                break\n",
    "        er_.append(val_error)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf5cd86-02d6-4866-b9bd-079201724cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = \"Google_Stock_Price_Train.csv\"\n",
    "raw = pd.read_csv(path + dataset, header=0, skiprows=[1])\n",
    "\n",
    "raw[\"day\"] = [t.day for t in pd.DatetimeIndex(raw.Date)]\n",
    "raw[\"month\"] = [t.month for t in pd.DatetimeIndex(raw.Date)]\n",
    "raw['year'] = [t.year for t in pd.DatetimeIndex(raw.Date)]\n",
    "raw.drop(['Date',\"day\",\"month\"],inplace=True,axis=1)\n",
    "\n",
    "raw['Volume'] = raw['Volume'].str.replace(',', '')\n",
    "raw['Close'] = raw['Close'].str.replace(',', '')\n",
    "\n",
    "data = raw.to_numpy(dtype='float')\n",
    "\n",
    "X = data[:,1:]\n",
    "y = data[:,:1]\n",
    "\n",
    "means = np.mean(y,axis=0)\n",
    "stds = np.std(y,axis=0)\n",
    "y = (y-means)/stds\n",
    "\n",
    "std_Ys = stds[0]\n",
    "mean_Ys = means[0]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "\n",
    "batch = 1\n",
    "steps = int(len(X_train) / batch)\n",
    "\n",
    "\n",
    "use_earlyStopFunction = True\n",
    "\n",
    "for f in [\"relu\",\"sigmoid\"]:\n",
    "    for n_ in [25,50,100]:\n",
    "        start = timer()\n",
    "        model_  = AndreTensor(X_train,y_train,batch,steps,n_input_nodes,n_,n_output_nodes,X_test,y_test,f,use_earlyStopFunction)\n",
    "        end = timer()\n",
    "        t_ = end - start\n",
    "        test_predicted = predict(tf.constant(X_test.astype(np.float32)), model_,f)\n",
    "        predictions = (test_predicted * std_Ys) + mean_Ys\n",
    "        target = (y_test * std_Ys) + mean_Ys\n",
    "        squareDeviation = computeSquareDeviation(predictions, target)\n",
    "        print(\"\\n\")\n",
    "        print(f,n_)\n",
    "        print(\"Time\",t_)\n",
    "        print(\"NRMSE\",np.sqrt(np.nanmean(squareDeviation)) / np.nanstd(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343f6499-bd87-4810-875e-c274736e2490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rings</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>weight.w</th>\n",
       "      <th>weight.s</th>\n",
       "      <th>weight.v</th>\n",
       "      <th>weight.sh</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.120</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rings  length  diameter  height  weight.w  weight.s  weight.v  weight.sh  \\\n",
       "0      7   0.350     0.265   0.090    0.2255    0.0995    0.0485      0.070   \n",
       "1      9   0.530     0.420   0.135    0.6770    0.2565    0.1415      0.210   \n",
       "2     10   0.440     0.365   0.125    0.5160    0.2155    0.1140      0.155   \n",
       "3      7   0.330     0.255   0.080    0.2050    0.0895    0.0395      0.055   \n",
       "4      8   0.425     0.300   0.095    0.3515    0.1410    0.0775      0.120   \n",
       "\n",
       "   sex  \n",
       "0    0  \n",
       "1    1  \n",
       "2    0  \n",
       "3    2  \n",
       "4    2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = \"Abalone.csv\"\n",
    "raw = pd.read_csv(path + dataset, header=0, skiprows=[1])\n",
    "map = {'M':0,'F':1,'I':2}\n",
    "raw = raw.replace({'sex':map})\n",
    "\n",
    "raw = raw.reindex(columns=['rings','length','diameter','height','weight.w','weight.s','weight.v','weight.sh','sex'])\n",
    "display(raw.head())\n",
    "\n",
    "data = raw.to_numpy(dtype='float')\n",
    "\n",
    "X = data[:,1:]\n",
    "y = data[:,:1]\n",
    "\n",
    "means = np.mean(y,axis=0)\n",
    "stds = np.std(y,axis=0)\n",
    "y = (y-means)/stds\n",
    "\n",
    "std_Ys = stds[0]\n",
    "mean_Ys = means[0]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "\n",
    "batch = 1\n",
    "steps = int(len(X_train) / batch)\n",
    "\n",
    "\n",
    "use_earlyStopFunction = True\n",
    "\n",
    "for f in [\"relu\",\"sigmoid\"]:\n",
    "    for n_ in [25,50,100]:\n",
    "        start = timer()\n",
    "        model_  = AndreTensor(X_train,y_train,batch,steps,n_input_nodes,n_,n_output_nodes,X_test,y_test,f,use_earlyStopFunction)\n",
    "        end = timer()\n",
    "        t_ = end - start\n",
    "        test_predicted = predict(tf.constant(X_test.astype(np.float32)), model_,f)\n",
    "        predictions = (test_predicted * std_Ys) + mean_Ys\n",
    "        target = (y_test * std_Ys) + mean_Ys\n",
    "        squareDeviation = computeSquareDeviation(predictions, target)\n",
    "        print(\"\\n\")\n",
    "        print(f,n_)\n",
    "        print(\"Time\",t_)\n",
    "        print(\"NRMSE\",np.sqrt(np.nanmean(squareDeviation)) / np.nanstd(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6d9612-b9b2-46c9-bc93-6c9207212860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 299 / 300\n",
      "\n",
      "relu 25\n",
      "Time 254.13177789999997\n",
      "NRMSE 0.012236692\n",
      " 299 / 300\n",
      "\n",
      "relu 50\n",
      "Time 261.5967908000001\n",
      "NRMSE 0.01706225\n",
      " 299 / 300\n",
      "\n",
      "relu 100\n",
      "Time 266.8086251\n",
      "NRMSE 0.009368287\n",
      " 299 / 300\n",
      "\n",
      "sigmoid 25\n",
      "Time 270.9827737999999\n",
      "NRMSE 0.064026564\n",
      " 299 / 300\n",
      "\n",
      "sigmoid 50\n",
      "Time 259.53861059999986\n",
      "NRMSE 0.06955298\n",
      " 299 / 300\n",
      "\n",
      "sigmoid 100\n",
      "Time 261.8416264000002\n",
      "NRMSE 0.07746383\n"
     ]
    }
   ],
   "source": [
    "#COVID DATASET\n",
    "dataset = \"covid_chile.csv\"\n",
    "raw = pd.read_csv(path + dataset, header=0, skiprows=[1])\n",
    "raw[\"day\"] = [t.day for t in pd.DatetimeIndex(raw.date)]\n",
    "raw[\"month\"] = [t.month for t in pd.DatetimeIndex(raw.date)]\n",
    "raw['year'] = [t.year for t in pd.DatetimeIndex(raw.date)]\n",
    "raw.drop([\"date\"],inplace=True,axis=1)\n",
    "raw = raw.reindex(columns=['confirmed','day','month','year','deaths','recovered'])\n",
    "\n",
    "\n",
    "\n",
    "data = raw.to_numpy(dtype='float')\n",
    "\n",
    "X = data[:,1:]\n",
    "y = data[:,:1]\n",
    "\n",
    "means = np.mean(y,axis=0)\n",
    "stds = np.std(y,axis=0)\n",
    "y = (y-means)/stds\n",
    "\n",
    "std_Ys = stds[0]\n",
    "mean_Ys = means[0]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "\n",
    "batch = 1\n",
    "steps = int(len(X_train) / batch)\n",
    "\n",
    "\n",
    "use_earlyStopFunction = False\n",
    "\n",
    "for f in [\"relu\",\"sigmoid\"]:\n",
    "    for n_ in [25,50,100]:\n",
    "        start = timer()\n",
    "        model_  = AndreTensor(X_train,y_train,batch,steps,n_input_nodes,n_,n_output_nodes,X_test,y_test,f,use_earlyStopFunction)\n",
    "        end = timer()\n",
    "        t_ = end - start\n",
    "        test_predicted = predict(tf.constant(X_test.astype(np.float32)), model_,f)\n",
    "        predictions = (test_predicted * std_Ys) + mean_Ys\n",
    "        target = (y_test * std_Ys) + mean_Ys\n",
    "        squareDeviation = computeSquareDeviation(predictions, target)\n",
    "        print(\"\\n\")\n",
    "        print(f,n_)\n",
    "        print(\"Time\",t_)\n",
    "        print(\"NRMSE\",np.sqrt(np.nanmean(squareDeviation)) / np.nanstd(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5788743-e879-4aab-b2a5-2ae877b11715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15 / 4891\n",
      "\n",
      "relu 25\n",
      "Time 214.50988310000002\n",
      "NRMSE 0.60192126\n",
      " 20 / 4891\n",
      "\n",
      "relu 50\n",
      "Time 285.7526662999999\n",
      "NRMSE 0.52774817\n",
      " 14 / 4891\n",
      "\n",
      "relu 100\n",
      "Time 201.3653757000002\n",
      "NRMSE 0.6197592\n",
      " 7 / 4891\n",
      "\n",
      "sigmoid 25\n",
      "Time 106.09863450000012\n",
      "NRMSE 0.92879117\n",
      " 9 / 4891\n",
      "\n",
      "sigmoid 50\n",
      "Time 133.03386109999974\n",
      "NRMSE 0.8584741\n",
      " 8 / 4891\n",
      "\n",
      "sigmoid 100\n",
      "Time 123.00270439999986\n",
      "NRMSE 0.8897776\n"
     ]
    }
   ],
   "source": [
    "#AVOCADOS\n",
    "\n",
    "avocados = pd.read_csv(path + 'filtered_avocados.csv')\n",
    "new_columns = pd.DataFrame(avocados[\"Date\"].str.split('-').tolist(), columns = ['yr', 'month', 'day'])\n",
    "\n",
    "# Change data type of new columns from str to int32\n",
    "new_columns[\"day\"] = new_columns[\"day\"].astype(np.int32)\n",
    "new_columns[\"month\"] = new_columns[\"month\"].astype(np.int32)\n",
    "# Attach new columns\n",
    "avocados[\"day\"] = new_columns[\"day\"]\n",
    "avocados[\"month\"] = new_columns[\"month\"]\n",
    "avocados[\"type\"]=pd.Categorical(avocados[\"type\"])\n",
    "avocados[\"region\"]=pd.Categorical(avocados[\"region\"])\n",
    "df_type = pd.get_dummies(avocados['type'], prefix = 'category')\n",
    "df_region = pd.get_dummies(avocados['region'], prefix = 'category')\n",
    "\n",
    "# Drop Date, type and region\n",
    "del avocados[\"Date\"]\n",
    "del avocados[\"type\"]\n",
    "del avocados[\"region\"]\n",
    "\n",
    "# Attach the one-hot encodings columns to other integer ones\n",
    "avocados_cleaned=pd.concat([avocados, df_type, df_region], axis=1, sort=False)\n",
    "\n",
    "# Shuffle data using the pandas function sample, with fraction as 1 since we want all data \n",
    "avocados = avocados_cleaned.sample(frac=1)\n",
    "# Converto to numpy array\n",
    "avocados = avocados.to_numpy()\n",
    "\n",
    "X = avocados[:,1:]\n",
    "y = avocados[:,:1]\n",
    "\n",
    "means = np.mean(y,axis=0)\n",
    "stds = np.std(y,axis=0)\n",
    "y = (y-means)/stds\n",
    "\n",
    "std_Ys = stds[0]\n",
    "mean_Ys = means[0]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "\n",
    "batch = 1\n",
    "steps = int(len(X_train) / batch)\n",
    "\n",
    "\n",
    "use_earlyStopFunction = True\n",
    "\n",
    "for f in [\"relu\",\"sigmoid\"]:\n",
    "    for n_ in [25,50,100]:\n",
    "        start = timer()\n",
    "        model_  = AndreTensor(X_train,y_train,batch,steps,n_input_nodes,n_,n_output_nodes,X_test,y_test,f,use_earlyStopFunction)\n",
    "        end = timer()\n",
    "        t_ = end - start\n",
    "        test_predicted = predict(tf.constant(X_test.astype(np.float32)), model_,f)\n",
    "        predictions = (test_predicted * std_Ys) + mean_Ys\n",
    "        target = (y_test * std_Ys) + mean_Ys\n",
    "        squareDeviation = computeSquareDeviation(predictions, target)\n",
    "        print(\"\\n\")\n",
    "        print(f,n_)\n",
    "        print(\"Time\",t_)\n",
    "        print(\"NRMSE\",np.sqrt(np.nanmean(squareDeviation)) / np.nanstd(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375d87bc-34d0-4018-8521-ff082d0f3117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>429.0</td>\n",
       "      <td>198</td>\n",
       "      <td>4341</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0  15.0          8         350.0         165    3693          11.5   \n",
       "1  18.0          8         318.0         150    3436          11.0   \n",
       "2  16.0          8         304.0         150    3433          12.0   \n",
       "3  17.0          8         302.0         140    3449          10.5   \n",
       "4  15.0          8         429.0         198    4341          10.0   \n",
       "\n",
       "   model year  origin  \n",
       "0          70       1  \n",
       "1          70       1  \n",
       "2          70       1  \n",
       "3          70       1  \n",
       "4          70       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 264 / 265\n",
      "\n",
      "relu 25\n",
      "Time 189.21876919999977\n",
      "NRMSE 0.42196378\n",
      " 264 / 265\n",
      "\n",
      "relu 50\n",
      "Time 189.71898310000006\n",
      "NRMSE 0.42817438\n",
      " 264 / 265\n",
      "\n",
      "relu 100\n",
      "Time 190.6256042\n",
      "NRMSE 0.42347154\n",
      " 264 / 265\n",
      "\n",
      "sigmoid 25\n",
      "Time 189.71132549999993\n",
      "NRMSE 0.44586417\n",
      " 264 / 265\n",
      "\n",
      "sigmoid 50\n",
      "Time 192.02405810000027\n",
      "NRMSE 0.430094\n",
      " 264 / 265\n",
      "\n",
      "sigmoid 100\n",
      "Time 194.91069620000007\n",
      "NRMSE 0.4809105\n"
     ]
    }
   ],
   "source": [
    "dataset = \"auto-mpg.csv\"\n",
    "raw = pd.read_csv(path + dataset, header=0, skiprows=[1])\n",
    "raw.drop([\"car name\"],inplace=True,axis=1)\n",
    "display(raw.head())\n",
    "\n",
    "data = raw.to_numpy(dtype='float')\n",
    "\n",
    "X = data[:,1:]\n",
    "y = data[:,:1]\n",
    "\n",
    "means = np.mean(y,axis=0)\n",
    "stds = np.std(y,axis=0)\n",
    "y = (y-means)/stds\n",
    "\n",
    "std_Ys = stds[0]\n",
    "mean_Ys = means[0]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "\n",
    "batch = 1\n",
    "steps = int(len(X_train) / batch)\n",
    "\n",
    "\n",
    "use_earlyStopFunction = False\n",
    "\n",
    "for f in [\"relu\",\"sigmoid\"]:\n",
    "    for n_ in [25,50,100]:\n",
    "        start = timer()\n",
    "        model_  = AndreTensor(X_train,y_train,batch,steps,n_input_nodes,n_,n_output_nodes,X_test,y_test,f,use_earlyStopFunction)\n",
    "        end = timer()\n",
    "        t_ = end - start\n",
    "        test_predicted = predict(tf.constant(X_test.astype(np.float32)), model_,f)\n",
    "        predictions = (test_predicted * std_Ys) + mean_Ys\n",
    "        target = (y_test * std_Ys) + mean_Ys\n",
    "        squareDeviation = computeSquareDeviation(predictions, target)\n",
    "        print(\"\\n\")\n",
    "        print(f,n_)\n",
    "        print(\"Time\",t_)\n",
    "        print(\"NRMSE\",np.sqrt(np.nanmean(squareDeviation)) / np.nanstd(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7810b-4b60-4fd1-8137-c41c1494ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NYC DATASET\n",
    "\n",
    "dataset = \"nyc_taxi.csv\"\n",
    "raw = pd.read_csv(path + dataset, header=0, skiprows=[1])\n",
    "\n",
    "\n",
    "raw[\"hour\"] = [t.hour for t in pd.DatetimeIndex(raw.timestamp)]\n",
    "raw[\"minute\"] = [t.minute for t in pd.DatetimeIndex(raw.timestamp)]\n",
    "raw[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(raw.timestamp)]\n",
    "raw[\"month\"] = [t.month for t in pd.DatetimeIndex(raw.timestamp)]\n",
    "raw['year'] = [t.year for t in pd.DatetimeIndex(raw.timestamp)]\n",
    "raw.drop(['timestamp'],inplace=True,axis=1)\n",
    "\n",
    "data = raw.to_numpy()\n",
    "\n",
    "X = data[:,1:]\n",
    "y = data[:,:1]\n",
    "\n",
    "means = np.mean(y,axis=0)\n",
    "stds = np.std(y,axis=0)\n",
    "y = (y-means)/stds\n",
    "\n",
    "std_Ys = stds[0]\n",
    "mean_Ys = means[0]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "\n",
    "batch = 1\n",
    "steps = int(len(X_train) / batch)\n",
    "\n",
    "\n",
    "use_earlyStopFunction = True\n",
    "\n",
    "for f in [\"relu\",\"sigmoid\"]:\n",
    "    for n_ in [25,50,100]:\n",
    "        start = timer()\n",
    "        model_  = AndreTensor(X_train,y_train,batch,steps,n_input_nodes,n_,n_output_nodes,X_test,y_test,f,use_earlyStopFunction)\n",
    "        end = timer()\n",
    "        t_ = end - start\n",
    "        test_predicted = predict(tf.constant(X_test.astype(np.float32)), model_,f)\n",
    "        predictions = (test_predicted * std_Ys) + mean_Ys\n",
    "        target = (y_test * std_Ys) + mean_Ys\n",
    "        squareDeviation = computeSquareDeviation(predictions, target)\n",
    "        print(\"\\n\")\n",
    "        print(f,n_)\n",
    "        print(\"Time\",t_)\n",
    "        print(\"NRMSE\",np.sqrt(np.nanmean(squareDeviation)) / np.nanstd(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fac59-cc84-4084-9d5f-3841fbf1956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIKE DATASET\n",
    "dataset = \"train_bikeDemand.csv\"\n",
    "raw = pd.read_csv(path + dataset, header=0, skiprows=[1])\n",
    "raw.drop(raw.columns[0], axis=1,inplace=True)\n",
    "cols = list(raw.columns)\n",
    "a, b = cols.index('holiday'), cols.index('count')\n",
    "cols[b], cols[a] = cols[a], cols[b]\n",
    "raw = raw[cols]\n",
    "\n",
    "data = raw.to_numpy(dtype='float')\n",
    "\n",
    "X = data[:,1:]\n",
    "y = data[:,:1]\n",
    "\n",
    "means = np.mean(y,axis=0)\n",
    "stds = np.std(y,axis=0)\n",
    "y = (y-means)/stds\n",
    "\n",
    "std_Ys = stds[0]\n",
    "mean_Ys = means[0]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "\n",
    "batch = 1\n",
    "steps = int(len(X_train) / batch)\n",
    "\n",
    "\n",
    "use_earlyStopFunction = True\n",
    "\n",
    "for f in [\"relu\",\"sigmoid\"]:\n",
    "    for n_ in [25,50,100]:\n",
    "        start = timer()\n",
    "        model_  = AndreTensor(X_train,y_train,batch,steps,n_input_nodes,n_,n_output_nodes,X_test,y_test,f,use_earlyStopFunction)\n",
    "        end = timer()\n",
    "        t_ = end - start\n",
    "        test_predicted = predict(tf.constant(X_test.astype(np.float32)), model_,f)\n",
    "        predictions = (test_predicted * std_Ys) + mean_Ys\n",
    "        target = (y_test * std_Ys) + mean_Ys\n",
    "        squareDeviation = computeSquareDeviation(predictions, target)\n",
    "        print(\"\\n\")\n",
    "        print(f,n_)\n",
    "        print(\"Time\",t_)\n",
    "        print(\"NRMSE\",np.sqrt(np.nanmean(squareDeviation)) / np.nanstd(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d280e9-ff68-47a5-9cdd-387d5211d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REC CENTER DATASET\n",
    "dataset = \"rec-center-hourly.csv\"\n",
    "raw = pd.read_csv(path + dataset, header=0, skiprows=[1,2])\n",
    "\n",
    "raw[\"day\"] = [t.day for t in pd.DatetimeIndex(raw.timestamp)]\n",
    "raw[\"month\"] = [t.month for t in pd.DatetimeIndex(raw.timestamp)]\n",
    "raw.drop([\"timestamp\"],inplace=True,axis=1)\n",
    "display(raw.head())\n",
    "\n",
    "data = raw.to_numpy(dtype='float')\n",
    "\n",
    "\n",
    "X = data[:,1:]\n",
    "y = data[:,:1]\n",
    "\n",
    "means = np.mean(y,axis=0)\n",
    "stds = np.std(y,axis=0)\n",
    "y = (y-means)/stds\n",
    "\n",
    "std_Ys = stds[0]\n",
    "mean_Ys = means[0]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "\n",
    "batch = 1\n",
    "steps = int(len(X_train) / batch)\n",
    "\n",
    "\n",
    "use_earlyStopFunction = True\n",
    "\n",
    "for f in [\"relu\",\"sigmoid\"]:\n",
    "    for n_ in [25,50,100]:\n",
    "        start = timer()\n",
    "        model_  = AndreTensor(X_train,y_train,batch,steps,n_input_nodes,n_,n_output_nodes,X_test,y_test,f,use_earlyStopFunction)\n",
    "        end = timer()\n",
    "        t_ = end - start\n",
    "        test_predicted = predict(tf.constant(X_test.astype(np.float32)), model_,f)\n",
    "        predictions = (test_predicted * std_Ys) + mean_Ys\n",
    "        target = (y_test * std_Ys) + mean_Ys\n",
    "        squareDeviation = computeSquareDeviation(predictions, target)\n",
    "        print(\"\\n\")\n",
    "        print(f,n_)\n",
    "        print(\"Time\",t_)\n",
    "        print(\"NRMSE\",np.sqrt(np.nanmean(squareDeviation)) / np.nanstd(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
